---
title: "STA 561 HW2 (Decision Trees)"
author: "Daniel Truver"
date: "1/29/2018"
header-includes:
  - \usepackage{amsmath}
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##### (1) Classifiers for Basketball Courts  

```{r Perceptron}
perceptron = function(X, y, I){
  iteration = 1
  w = rep(0, ncol(X))
  for (j in 1:I){
    for (i in seq_along(y)){
      if (y[i]*(w %*% X[i,]) <= 0){
        w = w + y[i]*X[i,] # update step
        iteration = iteration + 1
      }
    }
    accuracy = sum((X %*% w) * y > 0)/length(y) # calculate propotion of correctly classified point
    if (accuracy == 1){ 
      break # no need to continue if we have perfect separation
    } 
  }
  return(list("iteration" = iteration, "w" = w, "accuracy" = accuracy))
}
```

###### (a) Let's run the perceptron because it's fun. 

```{r enteringB-BallData}
X_1 = c(.75, .85, .85, .15,.05,.05,.85)
X_2 = c(.1,.8,.95,.1,.25,.5,.25)
Y = c(-1, -1, 1, -1, 1, 1, -1)
X_b = cbind(X_1, X_2)
```

```{r perceptron}
res = perceptron(X_b, Y, I = 100)
```

The preceptron made `r res$iteration` mistakes before converging with accuracy = `r res$accuracy` (error = `r 1-res$accuracy`). See the decision boundary below. 

```{r perceptronDecisionBound}
suppressMessages(library(ggplot2))
ggplot(data = data.frame(X_1, X_2, Y), aes(x = X_1, y = X_2)) +
  geom_point(aes(pch = factor(Y)), size = 7) +
  scale_shape_manual(values = c("-", "+")) +
  geom_abline(slope = -res$w[["X_1"]]/res$w[["X_2"]], intercept = 0) +
  ggtitle("Result of Perceptron Algorithm",
          subtitle = "Other possible separators featured in color") +
  geom_abline(intercept = 0, slope = 1, color = "red", lty = "dashed") + 
  geom_abline(intercept = 0, slope = 1.05, color = "blue", lty = "dashed") + 
  theme_bw() + 
  theme(plot.subtitle = element_text(color = "red"))
```  

###### (b) Growing the decision tree.

```{r testGrow}
gini.index = function(node){ # function to calculate gini Index
  p = sum(node > 0)/length(node)
  I = 2*p*(1-p)
  return(I)
}
X = X_b
Y = Y
NODE = list(c(1,2,3,4,7))
grow.tree = function(X, Y, current_node = NULL, 
                     epsilon = 10^-6, threshold = 0.20){ # split node based on gini index
  X = data.frame(X)
  Y = data.frame(Y = Y)
  data.df = cbind(X,Y)
  if (is.null(current_node)){
    current_node = data.df
  }
  # variables to be filled later
  delta.I.record = c()
  leaf1.record = list()
  leaf2.record = list()
  split.record = list()
  c = 1
  for ( i in 1:(ncol(current_node)-1) ){
    # get possible split values, adjust min, max up, down to make inclusive inequalities 
    # run without trouble
    split.sequence = sort(unique(current_node[,i]))
    split.sequence[1] = split.sequence[1] + epsilon
    split.sequence[length(split.sequence)] = split.sequence[length(split.sequence)] - 
      epsilon
    for(x in split.sequence){ # this loop calculates each possible split and reduction in gini index
      split.record = append(split.record, list(c(i, x)))
      node_1 = current_node[which(current_node[,i] <= x),]
      leaf1.record[[c]] = node_1
      I_1 = gini.index(node = node_1$y)
      node_2 = current_node[which(current_node[,i] > x),]
      leaf2.record[[c]] = node_2
      I_2 = gini.index(node = node_2$y)
      delta.I = gini.index(current_node$y) - sum(nrow(node_1)/nrow(current_node)*I_1,
                                                 nrow(node_2)/nrow(current_node)*I_2)
      delta.I.record = c(delta.I.record, delta.I)
      c = c + 1
    }
  }
  optimal = which.max(delta.I.record) # find split with greatest gini reduction
  new.nodes = list(leaf1.record[[optimal]], leaf2.record[[optimal]])
  Split = split.record[[optimal]]
  return(list("opt" = delta.I.record[optimal], "leaves" = new.nodes, "split" = Split,
              "threshold" = delta.I.record[optimal] > threshold))
}
```

```{r irrelevantTesting, include=FALSE}
# for (current_node in NODES){
#   new.nodes = list()
#   SPLITS = list()
#   delta.I.record = c()
#   leaf1.record = list()
#   leaf2.record = list()
#   split.record = list()
#   c = 1
#   for (i in 1:ncol(X)){
#     split.sequence = sort(unique(X[,i][current_node]))
#     split.sequence[1] = split.sequence[1] + epsilon
#     split.sequence[length(split.sequence)] = split.sequence[length(split.sequence)] - 
#       epsilon
#     for(x in split.sequence){
#       split.record = append(split.record, list(c(i, x)))
#       node_1 = which(X[,i] <= x)
#       leaf1.record[[c]] = node_1
#       I_1 = gini.index(node = y[node_1])
#       node_2 = which(X[,i] > x)
#       leaf2.record[[c]] = node_2
#       I_2 = gini.index(node = y[node_2])
#       delta.I = gini.index(y[current_node]) - sum(length(node_1)/length(current_node)*I_1,
#                                                length(node_2)/length(current_node)*I_2)
#       delta.I.record = c(delta.I.record, delta.I)
#       c = c + 1
#     }
#   }
#   optimal = which.max(delta.I.record)
#   new.nodes = append(new.nodes, 
#                      list(leaf1.record[[optimal]], leaf2.record[[optimal]]))
#   SPLITS = append(SPLITS, split.record[[optimal]])
# }
```

The function defined above splits a single node based on the gini index. We implement it below, step by step, until we have grown a full tree.

```{r growingFullTree}
tree_1 = grow.tree(X,Y)
pure_1 = length(unique(tree_1$leaves[[1]]$y)) == 1
pure_2 = length(unique(tree_1$leaves[[2]]$y)) == 1
split_1 = tree_1$split
# we see that the second leaf is not pure, so we attempt to split it again
tree_2 = grow.tree(X, Y, current_node = tree_1$leaves[[2]])
pure_1 = length(unique(tree_2$leaves[[1]]$y)) == 1
pure_2 = length(unique(tree_2$leaves[[2]]$y)) == 1
split_2 = tree_2$split
# all nodes are now pure; we should stop
```

We can see the splits in this lovely graphic. 

```{r plottingTheTree, echo=FALSE}
ggplot(data = data.frame(X_1, X_2, Y), aes(x = X_1, y = X_2)) +
  geom_point(aes(pch = factor(Y)), size = 7) +
  scale_shape_manual(values = c("-", "+")) +
  geom_vline(xintercept = tree_1$split[2], lty = "dotted") +
  geom_text(aes(x = tree_1$split[2]+.05, y = 0.7, label = "X_1 <= .05")) +
  geom_line(data=data.frame(x = c(tree_1$split[2],1),y = tree_2$split[2]), aes(x,y), lty = "dotted") +
  geom_text(aes(x = 0.5, y = tree_2$split[2]+.05, label = "X_2 > 0.80")) +
  theme_bw() +
  ggtitle("Decision Tree Splits on Basketball Court Data",
          subtitle = "Equal Accuracy Decision Tree (for this dataset) in red") +
  geom_vline(xintercept = 0.075, color = "red", lty = "dotted") +
  geom_line(data = data.frame(x = c(.075,1),y = .825), aes(x,y), color = "red", lty = "dotted") +
  theme(plot.subtitle = element_text(color = "red"))
```

The error on this tree is 0. We should note that if we set the $\Delta I$ threshold at or above $0.27$, then there are not splits in the tree; we get only one class. 

###### (c) The optimal linear classfier

If we want to minimize the expected loss, we need to minimize the area between the true boundary, $\sqrt{x_1}$ and the decision boundary $w\cdot x_1$. Therefore, we compute the following integral.  

$$
\begin{aligned}
h(w) 
&= \int_0^1\min(|\sqrt{x_1} - wx|, 1-\sqrt{x_1})dx_1 \\
&= \int_0^{1/w^2} (\sqrt{x_1}-wx_1)dx_1 + \int_{1/w^2}^{1/w}(wx_1-\sqrt{x_1})dx_1 + \int_{1/w}^1(1-\sqrt{x_1})dx_1 \\
&= \frac{1}{3}w^{-3} - \frac{1}{2}w^{-1} + \frac{1}{3}
\end{aligned}
$$

To get the minimum, we take the derivative, set that baby equal to zero, and crank the algebra machine until we get: 

$$
\begin{aligned}
& 0 = h'(w) = -w^{-4} + \frac{1}{2}w^{-2} \\
&\implies w = 2^{1/2} = 1.414
\end{aligned}
$$

We plot the optimal decision boundary here. 

```{r plottingOptimal, echo=FALSE}
x_1.plot = seq(0,1, length.out = 100)
x_2.plot = sqrt(x_1.plot)
ggplot(data = data.frame(X_1, X_2, Y), aes(x = X_1, y = X_2)) +
  geom_point(aes(pch = factor(Y)), size = 7) +
  scale_shape_manual(values = c("-", "+")) +
  geom_abline(slope = 1.414, intercept = 0) + 
  geom_line(data = data.frame(x_1.plot, x_2.plot), aes(x = x_1.plot, y = x_2.plot),
            lty = "dotted")+
  geom_abline(slope = -res$w[["X_1"]]/res$w[["X_2"]], intercept = 0, 
              color = "red", lty = "dashed") +
  theme_bw() +
  ggtitle("Optimal Linear Classifier",
          subtitle = "Our perceptron decision boundary in red") +
  theme(plot.subtitle = element_text(color = "red"))
  xlim(0,1) +
  ylim(0,1)
```

We note that this classifier has lower accuracy in than the perceptron separator we found in a previous part. This is not surprising; the optimal classifier is the one that should perform best on average over a set of datasets, whereas our perceptron trained on one dataset specifically.  

###### (d) The optimal decision tree

The general idea behind our approach here is the same as in optimizing the linear classifier. Since we are assuming the location of shots to be uniformly distributed, we want to minimize the misclassified area of the court. We begin with a visulalization. A general decision tree that splits on $X_1$ and then $X_2$ looks something like this. 

```{r generalDecisionTree, echo=FALSE}
#points for tree boundaries 
S1 = data.frame(x_1 = c(.5,.5), x_2 = c(0,1))
S2 = data.frame(x_1 = c(0,.5), x_2 = c(.5,.5))
S3 = data.frame(x_1 = c(.5,1), x_2 = c(.875,.875))
ggplot(data = data.frame(x_1.plot, x_2.plot), aes(x = x_1.plot, y = x_2.plot)) +
  geom_line() +
  geom_line(data = S1, aes(x_1, x_2), lty = "dashed") +
  geom_line(data = S2, aes(x_1, x_2), lty = "dashed") +
  geom_line(data = S3, aes(x_1, x_2), lty = "dashed") +
  geom_text(data = data.frame(x = c(.25,.25,.75,.75), y = c(.25,.75,.5,.95)),
            aes(x = x, y = y, label = c("(-)","(+)","(-)","(+)")),
            size = 14, color = "blue") +
  geom_text(data = data.frame(x = c(.05,.42,.57,.95), y =c(.375,.55,.8,.92)),
            aes(x=x, y =y, label = "PROBLEM")) +
  geom_text(aes(x = -.02, y = .5, label = "s_2")) +
  geom_text(aes(x = .5, y = 0, label = "s_1")) +
  geom_text(aes(x = 1.02, y = .875, label = "s_3")) +
  theme_bw() +
  ggtitle("General flavor of decision tree split on X_1 first")
```

Here, $s_1$ is the $x_1$ intercept of the vertical split, and $s_2, s_3$ are the $x_2$ intercepts of the left and right horizontal splits, respectively. Classification of each region is given by the obnoxiously large positive and negative signs. The ares that would be misclassified have a large 'PROBLEM' label. 

We can calculate this area as follows:
$$
\begin{aligned}
h(s_1, s_2, s_3) 
&= \int_0^{s_1} |\sqrt{x_1} - s_2|dx_1 + \int_{s_1}^1 |\sqrt{x_1}-s_3|dx_1 \\
&= \int_0^{s_2^2}(s_2 - \sqrt{x_1})dx_1 + \int_{s_2^2}^{s_1}(\sqrt{x_1}-s_2)dx_1 \\
&\quad\quad +\int_{s_1}^{s_3^2}(s_3 - \sqrt{x_1})dx_1 + \int_{s_3^2}^1(\sqrt{x_1}-s_3)dx_1 \\
&\text{...some algebra later...} \\
&= \frac{2}{3}s_2^3 + \frac{2}{3}s_3^3 + \frac{4}{3}s_1^{3/2} - s_2s_1 -s_3s_1 -s_3 + \frac{2}{3}
\end{aligned}
$$

Now, we could take some partial derivatives, set things equal to zero, do more algebra, pray for death, and get an honest to (your preferred deity here) expression for these variables. Or, in less than one second, we run the following code chunk and get an approximation.

```{r exhaustiveMinimization}
s1 = seq(0,1, length.out = 100)
s2 = seq(0,1, length.out = 100)
s3 = seq(0,1, length.out = 100)
split.exhaustive = matrix(NA, nrow = 100^3, ncol = 4)
i = 1
for (x in s1){
  for (y in s2){
    for (z in s3){
      split.exhaustive[i, 1] = x
      split.exhaustive[i, 2] = y
      split.exhaustive[i, 3] = z
      split.exhaustive[i, 4] = (2/3)*y^3 + (2/3)*z^3 + (4/3)*x^(3/2) -y*x -z*x - z + 2/3
      i = i + 1
    }
  }
}
minimum = which.min(split.exhaustive[,4])
s.opt = split.exhaustive[minimum, 1:3]
```

```{r tableForSopt, echo=FALSE}
rm(split.exhaustive)
knitr::kable(data.frame(s_1 = s.opt[1], s_2 = s.opt[2], s_3 = s.opt[3]),
             caption = "Optimal Splitting Values", digits = 2)
```  

Please enjoy the following graphic. Signed copies available upon request.

```{r optimalTree1Visual, echo=FALSE}
ggplot(data = data.frame(X_1, X_2, Y), aes(x = X_1, y = X_2)) +
  geom_point(aes(pch = factor(Y)), size = 7) +
  scale_shape_manual(values = c("-","+")) +
  geom_vline(xintercept = s.opt[1]) +
  geom_line(data = data.frame(x = c(0,s.opt[1]), y = s.opt[2]), aes(x,y)) +
  geom_line(data = data.frame(x = c(s.opt[1], 1), y = s.opt[3]), aes(x,y)) +
  ggtitle("Theoretical Optimal Decision Tree",
          subtitle = "Our grown decision tree in red") +
  geom_vline(xintercept = tree_1$split[2], color = "red", lty = "dashed") +
  geom_line(data=data.frame(x = c(tree_1$split[2],1),y = tree_2$split[2]), 
            aes(x,y), lty = "dashed", color = "red") +
  geom_line(data = data.frame(x_1.plot, x_2.plot), aes(x_1.plot, x_2.plot),
            lty = "dotted") +
  theme_bw() +
  theme(plot.subtitle = element_text(color = "red")) 
```

Once again, we see that the accuracy is higher on our grown tree for this particular dataset. The theoretically optimal tree only has lower expected loss on average.  

Now, we move on to the case of splitting on $X_2$ first. The general case looks something like this:

```{r generalTree2, echo=FALSE}
ggplot(data=data.frame(x_1.plot, x_2.plot), aes(x_1.plot, x_2.plot)) +
  geom_line(lty = "dotted") +
  geom_hline(yintercept = .5) +
  geom_line(data = data.frame(x = .125, y = c(0,.5)), aes(x,y)) +
  geom_line(data = data.frame(x = .6, y = c(.5,1)), aes(x,y)) +
  geom_text(data = data.frame(x = c(.05,.1, .8,.8), y = c(.25,.75,.25,.75)),
            aes(x,y, label = c("(+)","(+)", "(-)","(-)")), color = "blue", size = 10) +
  geom_text(data = data.frame(x = c(0, .125, .6), y = c(.55, -.02, 1.02)), 
            aes(x,y, label = c("s_1", "s_2", "s_3"))) +
  ggtitle("General flavor of decision tree split on X_2 first") +
  theme_bw()
```  

We will go through the same process as before to obtain an expression for the expected loss. 
$$
\begin{aligned}
h(s_1, s_2, s_3)
&= \int_0^{s_2} \sqrt{x_1}dx_1 + \int_{s_2}^{s_1^2}(s_1 - \sqrt{x_1})dx_1 + \int_{s_1}^{s_3}(\sqrt{x_1} - s_1) dx_1+ \int_{s_3}^1(1-\sqrt{x_1})dx_1\\
&\text{...some algegra later...}\\
&= \frac{4}{3}s_2^{3/2} + \frac{4}{3}s_3^{3/2} + \frac{1}{3}s_1^3 - s_1s_2 -s_1s_3-s_3 + \frac{1}{3}
\end{aligned}
$$

Again, we will compute the minimum of this function with this lovely computing machine exactly as above to obtain the following values for $s_1, s_2, s_3$. 

```{r exhaustiveTree2, echo=FALSE}
split.exhaustive = matrix(NA, nrow = 100^3, ncol = 4)
i = 1
for (x in s1){
  for (y in s2){
    for (z in s3){
      split.exhaustive[i, 1] = x
      split.exhaustive[i, 2] = y
      split.exhaustive[i, 3] = z
      split.exhaustive[i, 4] = (4/3)*y^(3/2) + (4/3)*z^(3/2) + (2/3)*x^(3) -y*x -z*x - z + 1/3
      i = i + 1
    }
  }
}
minimum = which.min(split.exhaustive[,4])
s.opt = split.exhaustive[minimum, 1:3]
knitr::kable(data.frame(s_1 = s.opt[1], s_2 = s.opt[2], s_3 = s.opt[3]),
             caption = "Optimal values for tree split on X_2 first", digits = 2)
```

Visually, it looks something like this.

```{r plottingOptTree2, echo=FALSE}
ggplot(data = data.frame(X_1, X_2, Y), aes(x = X_1, y = X_2)) +
  geom_point(aes(pch = factor(Y)), size = 7) +
  scale_shape_manual(values = c("-","+")) +
  geom_hline(yintercept = s.opt[1]) +
  geom_line(data = data.frame(x = s.opt[2], y = c(0,s.opt[1])), aes(x,y)) +
  geom_line(data = data.frame(x = s.opt[3], y = c(s.opt[1], 1)), aes(x,y)) +
  geom_line(data = data.frame(x_1.plot, x_2.plot), aes(x_1.plot, x_2.plot),
            lty = "dotted") +
  ggtitle("Theoretical Optimal Decision Tree",
          subtitle = "Our grown decision tree in red") +
  geom_vline(xintercept = tree_1$split[2], color = "red", lty = "dashed") +
  geom_line(data=data.frame(x = c(tree_1$split[2],1),y = tree_2$split[2]), 
            aes(x,y), lty = "dashed", color = "red") +
  geom_line(data = data.frame(x_1.plot, x_2.plot), aes(x_1.plot, x_2.plot),
            lty = "dotted") +
  theme_bw() +
  theme(plot.subtitle = element_text(color = "red")) 
``` 

Once again, the accuracy is lower for the optimal loss tree on this specific example. We are still not troubled by this for the reasons explained above. 

NOTE: In the above integrals, we have assumed each time that $s_2 < s_3$. We are comfortable making this decision since moving the $s_2$ and $s_3$ lines past each other always results in more misclassified area. Namely, an arrangement such as this will have the $s_2$ or $s_3$ line intersect the $s_1$ line without intersecting the true decision boundary. This will yield an additional rectangle of misclassified area that need not exist.  

###### (e) Transformations of Covariates  

Well, this one seems fairly tame after the algebra monstrosity of part (d). We know the true decision boundary to be $x_2 = \sqrt{x_1}$, so $x_2^2 = x_1$, and we consider the transformation, $t:x_2 \rightarrow x_2^2$. 

```{r transformPlot, echo=FALSE}
ggplot(data = data.frame(X_1, X_2 = X_2^2, Y), aes(X_1, X_2)) +
  geom_point(aes(pch = factor(Y)), size = 7) +
  scale_shape_manual(values = c("-", "+")) +
  geom_line(data = data.frame(x_1 = x_1.plot, x_2 = x_2.plot^2), aes(x_1, x_2)) +
  geom_abline(slope = 1, intercept = 0, color = "red", lty = "dashed", 
              size = 1.2, alpha = .8) +
  ggtitle("Transformation of x_2 to x_2^2",
          subtitle = "Perfect Classifier in red") +
  theme_bw() +
  theme(plot.subtitle = element_text(color = "red")) +
  xlab("t(X_2)")
```

Using the recognizing straight lines property, we deduce that the optimal classifier is the line of slope 1 passing through the origin under the transformation $t:x_2 \rightarrow x_2^2$. This classifier will always have zero error (perfect accuracy) because it is the same as the decision boundary.

###### (f) Decision tree under transformation

We could not obtain the same error under this transformation, especially not a depth 2 decision tree. A tree will produce a finite number of rectangular partitions of the space which are unable to capture a diagonal line. One could attempt an argument for 0 expected loss as the number of terminal nodes $c \to\infty$, but this is unrealistic.

###### (h) Classifying the paint (linear)

We will use the same integration technique as above. We consider below the line as inside the paint and above the line as outside the paint. We want to minimize the area
$$
\begin{aligned}
h(w) 
&= \int_0^{.5} wx_1 dx_1 + \int_{.5}^{.25/w} (.25-wx_1)dx_1 + \int_{.25/w}^{1} (wx_1 - .25)dx_1 \\
&\text{...some algebra later...}\\
&= \frac{1}{16}w^{-1} + \frac{1}{4}w + \frac{3}{8}
\end{aligned}
$$

Setting $h'(w) = 0$, we get 
$$
w = \sqrt{\frac{1}{12}}
$$

Which looks like

```{r paintLlinearPlot, echo=FALSE}
trueRisk = (1/16)/(sqrt(1/12)) + (3/4)*sqrt(1/12) - 3/8
ggplot(data = data.frame(x = c(.5,.5,1), y =c(0,.25,.25) ), aes(x,y)) +
  geom_line(lty = "dotted") +
  xlim(0,1) + 
  ylim(0,1) +
  geom_abline(intercept = 0, slope = sqrt(1/12)) +
  theme_bw() +
  ggtitle("Optimal Linear Classifier for the Paint",
          "Below line classified as +1")
``` 

The 'true risk' of this classfier is `r round(trueRisk, 2)`.

###### (i) Classifying the paint (tree)

Fortunately, the structure of the decision tree decision boundary makes this obvious.  

$$
f(x_1, x_2) = 
\begin{cases}
-1 & x_1\leq 0.5 \\
-1 & (x_1, x_2) \in (0.5,1] \times (0.25, 1]  \\
1 & (x_1, x_2) \in (0.5,1] \times [0, 0.25]
\end{cases}
$$

The decision boundary is as follows. 

```{r paintTreePlot}
ggplot(data = data.frame(x = c(.5,.5,1), y =c(0,.25,.25) ), aes(x,y)) +
  geom_line() +
  xlim(0,1) + 
  ylim(0,1) + 
  ggtitle("Paint Decision Boundary", 
          "Optimal Tree Decision Boundary in Red") +
  geom_vline(xintercept = 0.5, color = "red", lty = "dashed", size = 1, alpha = .65) +
  geom_line(data = data.frame(x_1 = c(0.5, 1), x_2 = .25), aes(x_1, x_2),
            color = "red", lty = "dashed", size = 1, alpha = .65) +
  theme_bw() +
  theme(plot.subtitle = element_text(color = "red")) +
  geom_text(aes(.75, .125, label = "(+1)"), size = 6, color = "red") +
  geom_text(aes(.75, .75, label = "(-1)"), size = 6, color = "red") +
  geom_text(aes(.25, .5, label = "(-1)"), size = 6, color = "red")
```  

The error is clearly 0.

******

##### (2) Variable Importance for Trees and Forests  



