---
title: "STA 561 HW2 (Decision Trees)"
author: "Daniel Truver"
date: "1/29/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##### (1) Classifiers for Basketball Courts  

```{r Perceptron}
perceptron = function(X, y, I){
  iteration = 1
  w = rep(0, ncol(X))
  for (j in 1:I){
    for (i in seq_along(y)){
      if (y[i]*(w %*% X[i,]) <= 0){
        w = w + y[i]*X[i,] # update step
        iteration = iteration + 1
      }
    }
    accuracy = sum((X %*% w) * y > 0)/length(y) # calculate propotion of correctly classified point
    if (accuracy == 1){ 
      break # no need to continue if we have perfect separation
    } 
  }
  return(list("iteration" = iteration, "w" = w, "accuracy" = accuracy))
}
```

###### (a) Let's run the perceptron because it's fun. 

```{r enteringB-BallData}
X_1 = c(.75, .85, .85, .15,.05,.05,.85)
X_2 = c(.1,.8,.95,.1,.25,.5,.25)
Y = c(-1, -1, 1, -1, 1, 1, -1)
X_b = cbind(X_1, X_2)
```

```{r perceptron}
res = perceptron(X_b, Y, I = 100)
```

The preceptron made `r res$iteration` mistakes before converging with accuracy = `r res$accuracy` (error = `r 1-res$accuracy`). See the decision boundary below. 

```{r perceptronDecisionBound}
suppressMessages(library(ggplot2))
ggplot(data = data.frame(X_1, X_2, Y), aes(x = X_1, y = X_2)) +
  geom_point(aes(pch = factor(Y)), size = 7) +
  scale_shape_manual(values = c("-", "+")) +
  geom_abline(slope = -res$w[["X_1"]]/res$w[["X_2"]], intercept = 0) +
  ggtitle("Result of Perceptron Algorithm",
          subtitle = "Other possible separators featured in color") +
  geom_abline(intercept = 0, slope = 1, color = "red", lty = "dashed") + 
  geom_abline(intercept = 0, slope = 1.05, color = "blue", lty = "dashed") + 
  theme_bw() + 
  theme(plot.subtitle = element_text(color = "red"))
```  

###### (b) Growing the decision tree.

```{r testGrow}
gini.index = function(node){ # function to calculate gini Index
  p = sum(node > 0)/length(node)
  I = 2*p*(1-p)
  return(I)
}
X = X_b
Y = Y
NODE = list(c(1,2,3,4,7))
grow.tree = function(X, Y, current_node = NULL, 
                     epsilon = 10^-6, threshold = 0.20){ # split node based on gini index
  X = data.frame(X)
  Y = data.frame(Y = Y)
  data.df = cbind(X,y)
  if (is.null(current_node)){
    current_node = data.df
  }
  # variables to be filled later
  delta.I.record = c()
  leaf1.record = list()
  leaf2.record = list()
  split.record = list()
  c = 1
  for ( i in 1:(ncol(current_node)-1) ){
    # get possible split values, adjust min, max up, down to make inclusive inequalities 
    # run without trouble
    split.sequence = sort(unique(current_node[,i]))
    split.sequence[1] = split.sequence[1] + epsilon
    split.sequence[length(split.sequence)] = split.sequence[length(split.sequence)] - 
      epsilon
    for(x in split.sequence){ # this loop calculates each possible split and reduction in gini index
      split.record = append(split.record, list(c(i, x)))
      node_1 = current_node[which(current_node[,i] <= x),]
      leaf1.record[[c]] = node_1
      I_1 = gini.index(node = node_1$y)
      node_2 = current_node[which(current_node[,i] > x),]
      leaf2.record[[c]] = node_2
      I_2 = gini.index(node = node_2$y)
      delta.I = gini.index(current_node$y) - sum(nrow(node_1)/nrow(current_node)*I_1,
                                                 nrow(node_2)/nrow(current_node)*I_2)
      delta.I.record = c(delta.I.record, delta.I)
      c = c + 1
    }
  }
  optimal = which.max(delta.I.record) # find split with greatest gini reduction
  new.nodes = list(leaf1.record[[optimal]], leaf2.record[[optimal]])
  Split = split.record[[optimal]]
  return(list("opt" = delta.I.record[optimal], "leaves" = new.nodes, "split" = Split,
              "threshold" = delta.I.record[optimal] > threshold))
}
```

```{r irrelevantTesting, include=FALSE}
# for (current_node in NODES){
#   new.nodes = list()
#   SPLITS = list()
#   delta.I.record = c()
#   leaf1.record = list()
#   leaf2.record = list()
#   split.record = list()
#   c = 1
#   for (i in 1:ncol(X)){
#     split.sequence = sort(unique(X[,i][current_node]))
#     split.sequence[1] = split.sequence[1] + epsilon
#     split.sequence[length(split.sequence)] = split.sequence[length(split.sequence)] - 
#       epsilon
#     for(x in split.sequence){
#       split.record = append(split.record, list(c(i, x)))
#       node_1 = which(X[,i] <= x)
#       leaf1.record[[c]] = node_1
#       I_1 = gini.index(node = y[node_1])
#       node_2 = which(X[,i] > x)
#       leaf2.record[[c]] = node_2
#       I_2 = gini.index(node = y[node_2])
#       delta.I = gini.index(y[current_node]) - sum(length(node_1)/length(current_node)*I_1,
#                                                length(node_2)/length(current_node)*I_2)
#       delta.I.record = c(delta.I.record, delta.I)
#       c = c + 1
#     }
#   }
#   optimal = which.max(delta.I.record)
#   new.nodes = append(new.nodes, 
#                      list(leaf1.record[[optimal]], leaf2.record[[optimal]]))
#   SPLITS = append(SPLITS, split.record[[optimal]])
# }
```

The function defined above splits a single node based on the gini index. We implement it below, step by step, until we have grown a full tree.

```{r growingFullTree}
tree_1 = grow.tree(X,Y)
pure_1 = length(unique(tree_1$leaves[[1]]$y)) == 1
pure_2 = length(unique(tree_1$leaves[[2]]$y)) == 1
split_1 = tree_1$split
# we see that the second leaf is not pure, so we attempt to split it again
tree_2 = grow.tree(X, Y, current_node = tree_1$leaves[[2]])
pure_1 = length(unique(tree_2$leaves[[1]]$y)) == 1
pure_2 = length(unique(tree_2$leaves[[2]]$y)) == 1
split_2 = tree_2$split
# all nodes are now pure; we should stop
```

We can see the splits in this lovely graphic. 

```{r plottingTheTree, echo=FALSE}
ggplot(data = data.frame(X_1, X_2, Y), aes(x = X_1, y = X_2)) +
  geom_point(aes(pch = factor(Y)), size = 7) +
  scale_shape_manual(values = c("-", "+")) +
  geom_vline(xintercept = tree_1$split[2], lty = "dotted") +
  geom_text(aes(x = tree_1$split[2]+.05, y = 0.7, label = "X_1 <= .05")) +
  geom_hline(yintercept = tree_2$split[2], lty = "dotted") +
  geom_text(aes(x = 0.5, y = tree_2$split[2]+.05, label = "X_2 > 0.80")) +
  theme_bw() +
  ggtitle("Decision Tree Splits on Basketball Court Data",
          subtitle = "Equal Accuracy Decision Tree (for this dataset) in red") +
  geom_vline(xintercept = 0.075, color = "red", lty = "dotted") +
  geom_hline(yintercept = .825, color = "red", lty = "dotted") +
  theme(plot.subtitle = element_text(color = "red"))
```

The error on this tree is 0. We should note that if we set the $\Delta I$ threshold at or above $0.27$, then there are not splits in the tree; we get only one class. 

###### (c) The optimal linear classfier

If we want to minimize the expected loss, we need to minimize the area between the true boundary, $\sqrt{x_1}$ and the decision boundary $w\cdot x_1$. Therefore, we compute the following integral.  

$$
\begin{aligned}
h(w) 
&= \int_0^1\min(|\sqrt{x_1} - wx|, 1-\sqrt{x_1})dx_1 \\
&= \int_0^{1/w^2} (\sqrt{x_1}-wx_1)dx_1 + \int_{1/w^2}^{1/w}(wx_1-\sqrt{x_1})dx_1 + \int_{1/w}^1(1-\sqrt{x_1})dx_1 \\
&= \frac{1}{3}w^{-3} - \frac{1}{2}w^{-1} + \frac{1}{3}
\end{aligned}
$$

To get the minimum, we take the derivative, set that baby equal to zero, and crank the algebra machine until we get: 

$$
\begin{aligned}
& 0 = h'(w) = -w^{-4} + \frac{1}{2}w^{-2} \\
&\implies w = 2^{1/2} = 1.414
\end{aligned}
$$

We plot the optimal decision boundary here. 

```{r plottingOptimal, echo=FALSE}
x_1.plot = seq(0,1, length.out = 100)
x_2.plot = sqrt(x_1.plot)
ggplot(data = data.frame(X_1, X_2, Y), aes(x = X_1, y = X_2)) +
  geom_point(aes(pch = factor(Y)), size = 7) +
  scale_shape_manual(values = c("-", "+")) +
  geom_abline(slope = 1.414, intercept = 0) + 
  geom_line(data = data.frame(x_1.plot, x_2.plot), aes(x = x_1.plot, y = x_2.plot),
            lty = "dotted")+
  geom_abline(slope = -res$w[["X_1"]]/res$w[["X_2"]], intercept = 0, 
              color = "red", lty = "dashed") +
  theme_bw() +
  ggtitle("Optimal Linear Classifier",
          subtitle = "Our perceptron decision boundary in red") +
  xlim(0,1) +
  ylim(0,1)
```

We note that this classifier has lower accuracy in than the perceptron separator we found in a previous part. This is not surprising; the optimal classifier is the one that should perform best on average over a set of datasets, whereas our perceptron trained on one dataset specifically.  

###### (d) The optimal decision tree

The general idea behind our approach here is the same as in optimizing the linear classifier. Since we are assuming the location of shots to be uniformly distributed, we want to minimize the misclassified area of the court. We begin with a visulalization. A general decision tree that splits on $X_1$ and then $X_2$ looks something like this. 

```{r generalDecisionTree, echo=FALSE}
#points for tree boundaries 
S1 = data.frame(x_1 = c(.5,.5), x_2 = c(0,1))
S2 = data.frame(x_1 = c(0,.5), x_2 = c(.5,.5))
S3 = data.frame(x_1 = c(.5,1), x_2 = c(.875,.875))
ggplot(data = data.frame(x_1.plot, x_2.plot), aes(x = x_1.plot, y = x_2.plot)) +
  geom_line() +
  geom_line(data = S1, aes(x_1, x_2), lty = "dashed") +
  geom_line(data = S2, aes(x_1, x_2), lty = "dashed") +
  geom_line(data = S3, aes(x_1, x_2), lty = "dashed") +
  geom_text(data = data.frame(x = c(.25,.25,.75,.75), y = c(.25,.75,.5,.95)),
            aes(x = x, y = y, label = c("-","+","-","+")), size = 16) +
  geom_text(data = data.frame(x = c(.05,.42,.57,.95), y =c(.375,.55,.8,.92)),
            aes(x=x, y =y, label = "PROBLEM")) +
  theme_bw()
```

Here, $s_1$ is the $x_1$ intercept of the vertical split, and $s_2, s_3$ are the $x_2$ intercepts of the left and right horizontal splits, respectively. Classification of each region is given by the obnoxiously large positive and negative signs. The ares that would be misclassified have a large 'PROBLEM' label. 

We can calculate this area as follows:
$$
\begin{aligned}
h(s_1, s_2, s_3) 
&= \int_0^{s_1} |\sqrt{x_1} - s_2|dx_1 + \int_{s_1}^1 |\sqrt{x_1}-s_3|dx_1 \\
&= \int_0^{s_2^2}(s_2 - \sqrt{x_1})dx_1 + \int_{s_2^2}^{s_1}(\sqrt{x_1}-s_2)dx_1 \\
&\quad\quad +\int_{s_1}^{s_3^2}(s_3 - \sqrt{x_1})dx_1 + \int_{s_3^2}^1(\sqrt{x_1}-s_3)dx_1 \\
&= \text{...some calculus later...} \\
&= \frac{2}{3}s_2^3 + \frac{2}{3}s_3^3 + \frac{4}{3}s_1^{3/2} - s_2s_1 -s_3s_1 -s_3 + \frac{2}{3}
\end{aligned}
$$

```{r exhaustiveMinimization}
s1 = seq(0,1, length.out = 100)
s2 = seq(0,1, length.out = 100)
s3 = seq(0,1, length.out = 100)
split.exhaustive = matrix(NA, nrow = 100^3, ncol = 4)
i = 1
for (x in s1){
  for (y in s2){
    for (z in s3){
      split.exhaustive[i, 1] = x
      split.exhaustive[i, 2] = y
      split.exhaustive[i, 3] = z
      split.exhaustive[i, 4] = (2/3)*y^3 + (2/3)*z^3 + (4/3)*x^(3/2) -y*x -z*x - z + 2/3
      i = i + 1
    }
  }
}
minimum = which.min(split.exhaustive[,4])
```
